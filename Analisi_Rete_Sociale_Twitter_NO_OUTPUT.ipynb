{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Analisi Rete Sociale Twitter.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "0Lxo4XPiigiV",
        "DPzTE3KHfbAb"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysIcjI5Pbp1a"
      },
      "source": [
        "# Primo Progetto di Social Computing - Analisi della rete sociale di Twitter\n",
        "\n",
        "## Autori: \n",
        "\n",
        "*   Emanuele Lena - 142411\n",
        "*   Ilaria Fenos - 142494\n",
        "*   Massimiliano Baldo - 142296\n",
        "*   Simone Dalla Pietà - 141995\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5UZ6iC1c316"
      },
      "source": [
        "## Introduzione Generale\n",
        "L’obiettivo del progetto è di reperire una porzione della rete sociale del social network Twitter,per poi fare un’analisi applicando alcune delle più tipiche tecniche di studio dei grafi.Nel dettaglio, si intende:\n",
        "\n",
        "* Reperire i dati pubblici di 5 di profili di \n",
        "partenza, dei profili a loro direttamente correlati(followers e followed) e di ulteriori profili casuali (scelti secondo certi criteri spiegati inseguito)\n",
        "* Costruire un grafo che rappresenta la rete sociale, dove–i nodi sono i profili scaricati–gli archi (diretti) indicano una relazione di follower→following (“chi segue chi”)\n",
        "* Applicare le più comuni tecniche di analisi sul grafo, quali la visualizzazione del grafo,misura delle distanze e centralità, calcolo della copertura minima e la stima della “small-world-ness” del grafo.\n",
        "* Calcolo delle correlazioni tra le variabili calcolate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ejzx-Td3ddWi"
      },
      "source": [
        "## Preparazione\n",
        "### Librerie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2OEhgBQbdAb"
      },
      "source": [
        "import tweepy  # for gather data from twitter API endpoint\n",
        "import os\n",
        "import json\n",
        "import pprint\n",
        "import csv  \n",
        "import random as rn\n",
        "import math\n",
        "import networkx as nx # for storing data before saving it on graph\n",
        "import pandas as pd   # for storing data before saving it on graph\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pylab import rcParams\n",
        "import pickle\n",
        "from scipy import stats\n",
        "from google.colab import drive  # to have a sort of common filesystem for all the \n",
        "                                # group members\n",
        "from datetime import datetime   # to measure execution time of some functions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBo-pGXKd5uK"
      },
      "source": [
        "### Collegamento a Google Drive (per leggere e scrivere files)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BttyneFMdy0b"
      },
      "source": [
        "# mount google drive here to read users and relations data frame\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8sBiSPAd1Zz"
      },
      "source": [
        "# folder where I have the data\n",
        "# data_folder_2 = \"/content/drive/My Drive/Colab Notebooks/analisi-grafo-twitter/\"\n",
        "data_folder = \"/content/drive/My Drive/Colab Notebooks/data-project/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QE3xdEZ__NiN"
      },
      "source": [
        "# check everything is ok in folder\n",
        "\n",
        "# enter in folder\n",
        "os.chdir(data_folder)\n",
        "\n",
        "# create eventual missing folder\n",
        "from pathlib import Path\n",
        "Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
        "Path(\"raw_data\").mkdir(parents=True, exist_ok=True)\n",
        "Path(\"analysis_data\").mkdir(parents=True, exist_ok=True)\n",
        "Path(\"pyvise_graphs\").mkdir(parents=True, exist_ok=True)\n",
        "Path(\"networkx_graphs\").mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev-_jksreKLj"
      },
      "source": [
        "### Accesso alle API di Twitter con Tweepy "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwefMPRkeQ-a"
      },
      "source": [
        "# Twitter API Credentials\n",
        "api_key = \"your_key\"\n",
        "api_key_secret = \"your_secret_key\"\n",
        "access_token = \"\"\n",
        "access_token_secret = \"\"\n",
        "bearer_token = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGkf636jeX04"
      },
      "source": [
        "# Tweepy authentication\n",
        "auth = tweepy.OAuthHandler(api_key, api_key_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
        "if (api.verify_credentials):\n",
        "  print(\"Authentication success!\")\n",
        "else:\n",
        "  print(\"Got a error\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tirmb9qremIu"
      },
      "source": [
        "## Reperimento dei dati"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwSHfWR5e736"
      },
      "source": [
        "### Reperimento dei profili principali e dei profili direttamente connessi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT7ouKA6g7eI"
      },
      "source": [
        "Questa funzione viene usata da ciascuno di noi per reperire le informazioni sul/i proprio/i utente/i assegnato e tutti i suoi followers e following. Nel dettaglio vengono reperiti:\n",
        "\n",
        "* tutti i dati dell'utente\n",
        "* tutti i dati dei followers (e l'indicazione che ognuno di loro segue l'utente) \n",
        "* tutti i dati dei following (e l'indicazione che ognuno di loro è seguito dall'utente) \n",
        "\n",
        "I dati di tutti gli utenti vengono salvati in un data frame (pd.DataFrame) df_users, dove ogni dettaglio reperito corrisponde ad una colonna. In particolare possono essere interessanti le colonne:\n",
        "\n",
        "* `df_users[\"id\"]` <- id del profilo\n",
        "* `df_users[\"screen_name\" ]` <- screen name del profilo\n",
        "* `df_users[\"followers_count\"]` <- numero di followers\n",
        "\n",
        "I dati su \"chi segue chi\" sono invece rappresentati in un data frame (pd.DataFrame) df_relations, da 2 colonne: `df_relations[\"Follower\"]` e `df_relations[\"Followed\"]`.\n",
        "\n",
        "Ogni riga di df_relations indica che un certo utente - `df_relations[\"Following\"]` - segue un altro - `df_relations[\"Followed\"]` (Gli utenti sono rappresentati dai loro id).\n",
        "\n",
        "La funzione `save_users_informations` ritorna in output entrambi i data frame per un utente.\n",
        "\n",
        "Nel dettaglio, la funzione usa:\n",
        "\n",
        "* `api.get_user` per recuperare i dati dell'utente\n",
        "* `api.followers` e `api.friends` per recuperare i followers e followings (quest'ultime con `Cursor`, per gestire automaticamente il numero limitato di richieste per gli endpoint)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICLVZEBRhAVO"
      },
      "source": [
        "def get_user_and_neighbors(api: tweepy.API, user: str,  \n",
        "                         quantity: int = None):\n",
        "    \"\"\"\n",
        "    Gather a twitter profile's data and the onces of its follower, followed\n",
        "\n",
        "    :param api: the (initialized) tweepy twitter api istance to use\n",
        "    :param user: the name of the profile from whitch I want to gather data\n",
        "    :param df_users: the dataframe whene I will save users data\n",
        "    :param df_users: the dataframe whene I will follower-followed relations\n",
        "    :param quantity: the quantity of followers/followed I want to fetch\n",
        "\n",
        "    return: df_users, df_relations filled with the new data\n",
        "    \"\"\"\n",
        "\n",
        "    df_users: pd.DataFrame = pd.DataFrame()\n",
        "    df_relations: pd.DataFrame = pd.DataFrame()\n",
        "     \n",
        "    # request of the user\n",
        "    user_info = api.get_user(screen_name=user)._json\n",
        "    row = pd.json_normalize(user_info)\n",
        "    df_users = pd.concat([df_users, row])\n",
        "\n",
        "    user_id = user_info[\"id\"]\n",
        "\n",
        "    if quantity is None:\n",
        "      followers_count = user_info[\"followers_count\"]\n",
        "      friends_count = user_info[\"friends_count\"]\n",
        "    else: \n",
        "      followers_count = quantity\n",
        "      friends_count = quantity\n",
        "\n",
        "    counter = 0\n",
        "    max = followers_count\n",
        "\n",
        "    # request of user's followers\n",
        "    for item in tweepy.Cursor(api.followers, id=user).items(followers_count):\n",
        "        data_item = item._json\n",
        "        row_user = pd.json_normalize(data_item)\n",
        "        # item -> user\n",
        "        row_relation = pd.DataFrame([[data_item[\"id\"], user_id]], columns=[\"Follower\", \"Following\"])\n",
        "        df_users = pd.concat([df_users, row_user])\n",
        "        df_relations = pd.concat([df_relations, row_relation])\n",
        "\n",
        "        # debug count\n",
        "        perc = math.floor(counter/max*100)\n",
        "        print(str(perc) + \"% of the followers downloaded\")\n",
        "        counter = counter+1\n",
        "\n",
        "    counter = 0\n",
        "    max = friends_count\n",
        "\n",
        "    # request of user's followings\n",
        "    for item in tweepy.Cursor(api.friends, id=user).items(friends_count):\n",
        "\n",
        "      data_item = item._json\n",
        "      row_user = pd.json_normalize(data_item)\n",
        "      # user -> item\n",
        "      row_relation = pd.DataFrame([[user_id, data_item[\"id\"]]], columns=[\"Follower\", \"Following\"])\n",
        "      df_users = pd.concat([df_users, row_user])\n",
        "      df_relations = pd.concat([df_relations, row_relation])\n",
        "\n",
        "      # debug count\n",
        "      perc = math.floor(counter/max*100)\n",
        "      print(str(perc) + \"% of the friends downloaded\")\n",
        "      counter = counter+1\n",
        "\n",
        "    return df_users, df_relations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-BPzf4zhXXR"
      },
      "source": [
        "Reperimento dati (per uno dei 5 profili):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_jB5c6MhVJg"
      },
      "source": [
        "# call the created function to get user infos\n",
        "df_users_damiano10, df_relations_damiano10 = get_user_and_neighbors(api, \"damiano10\")\n",
        "# df_users, df_relations = get_user_and_neighbors(\"mizzaro\")\n",
        "# df_users, df_relations = get_user_and_neighbors(\"Iglu81\")\n",
        "# df_users, df_relations = get_user_and_neighbors(\"KevinRoitero\")\n",
        "# df_users, df_relations = get_user_and_neighbors(\"Miccighel_\")\n",
        "\n",
        "# save the datasets in csv\n",
        "df_users_damiano10.to_csv(data_folder + \"raw_data/df_users_damiano10.csv\", index=False)\n",
        "df_relations_damiano10.to_csv(data_folder + \"raw_data/df_relations_damiano10.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ofK5_7QfWt7"
      },
      "source": [
        "### Reperimento dei profili casuali aggiuntivi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auhSYNEshEmo"
      },
      "source": [
        "Le funzioni `random_followers` e `random_followings` permettono di reperire rispettivamente:\n",
        "\n",
        "* 10 followers casuali per 5 followers (anch'essi casuali) di un profilo \n",
        "* 10 followings casuali per 5 followings (anch'essi casuali) di un profilo \n",
        "\n",
        "Queste funzioni recuperano:\n",
        "\n",
        "* i dati dei nuovi utenti scovati\n",
        "* un'indicazione di quali sono i profili dai quali sono stati scovati questi nuovi utenti (chi sono i 5 followers/followed diretti del profilo da cui abbiamo trovato i 10*5 nuovi profili)\n",
        "\n",
        "I dati degli utenti scovati e le indicazioni sui profili da cui sono stati scoperti vengono rappresentati da 2 dataframe che seguno le stesse strutture descritte per `save_users_informations`.\n",
        "\n",
        "Questi dataframe vengono ritornati in output da entrambi le funzioni.\n",
        "\n",
        "Nel dettaglio, le funzioni usano:\n",
        "\n",
        "* `api.followers_ids` e `api.friends_ids` per reperire la lista di followers/followed casuali per i 5 followers/followed del profilo principale (di questi, poi si estrarranno 10 casuali). Queste funzioni sono state eseguite con `Cursor`, per gestire automaticamente la paginazione dei risultati.\n",
        "* `api.get_user` per recuperare i dati di ciascuno dei 10*5 (nuovi) profili casuali"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8j5bhrJEhJ8Z"
      },
      "source": [
        "def random_followers(\n",
        "                    api: tweepy.API,\n",
        "                    profile_screen_name: str, \n",
        "                    df_users: pd.DataFrame,\n",
        "                    df_relations: pd.DataFrame):\n",
        "  \"\"\"\n",
        "\n",
        "  :param api: the (initialized) tweepy twitter api istance to use\n",
        "  :param profile_screen_name: the screen name of the user from which I select the 5 randoms\n",
        "  :param df_users:\n",
        "  :param df_relations:\n",
        "  \n",
        "  return: df_random_followers, df_random_followers_relations\n",
        "  \"\"\"\n",
        "\n",
        "  # user_id = df_users.loc[0,\"id\"]\n",
        "  user_id = df_users.loc[df_users[\"screen_name\"] == profile_screen_name, \"id\"].to_numpy()[0]\n",
        "\n",
        "  #code to get 5 random followers\n",
        "\n",
        "  # get ids of all followers of user_id\n",
        "  df_followers = df_relations.loc[df_relations[\"Following\"] == user_id]\n",
        "\n",
        "  # get their data\n",
        "  df_followers = df_followers.join(df_users, lsuffix='Follower', rsuffix='id')\n",
        "\n",
        "  # select only the onces that has at least 10 followers\n",
        "  df_followers = df_followers.loc[df_followers[\"followers_count\"] >= 10]\n",
        "\n",
        "  # get 5 followers random\n",
        "  df_sample = df_followers.sample(n=5)\n",
        "  random_id = df_sample['id'].to_numpy()\n",
        "\n",
        "  print(\"Scelti i 5 casuali diretti\")\n",
        "\n",
        "  # print(random_id)\n",
        "  \n",
        "  #code to get 10 followers for each random follower\n",
        "  list_of_ten = [] #the list will have 50 id\n",
        "\n",
        "  # prepare the dataframe where i say who follows who\n",
        "  df_random_followers_relations = pd.DataFrame()\n",
        "\n",
        "  for user in random_id:\n",
        "\n",
        "    # followers = api.followers_ids(user)\n",
        "\n",
        "    followers = list()\n",
        "    \n",
        "    for flw in tweepy.Cursor(api.followers_ids, id=user).pages():\n",
        "      followers.extend(flw)\n",
        "\n",
        "    # print(followers)\n",
        "    \n",
        "    ten = rn.sample(followers, 10)\n",
        "\n",
        "    # print(ten)\n",
        "    for random_follower in ten:\n",
        "      list_of_ten.append(random_follower)\n",
        "      # random_follower -> user\n",
        "      row_relation = pd.DataFrame([[random_follower, user]], columns=[\"Follower\", \"Following\"])\n",
        "      df_random_followers_relations = pd.concat([df_random_followers_relations, row_relation])\n",
        "  \n",
        "  # print(list_of_ten)\n",
        "\n",
        "  print(\"Scelti i profili da scaricare\")\n",
        "  i=0\n",
        "\n",
        "  #code to create a dataframe with the users's info\n",
        "  df_random_followers = pd.DataFrame()\n",
        "\n",
        "  for item in list_of_ten:\n",
        "\n",
        "    user_info = api.get_user(id=item)._json\n",
        "    \n",
        "    # for user in tweepy.Cursor(api.get_user, id=item).items(1):\n",
        "    #  user_info = user._json\n",
        "    \n",
        "    row = pd.json_normalize(user_info)\n",
        "    df_random_followers = pd.concat([df_random_followers, row])\n",
        "\n",
        "    print(i, \"% dei profili scaricati\")\n",
        "    i = i+2\n",
        "\n",
        "  print(\"Operazione Completata!\")\n",
        "  # I return the new users data + the relations df that says who follows who\n",
        "  return df_random_followers, df_random_followers_relations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeeRmFl9hM92"
      },
      "source": [
        "def random_followings(\n",
        "                    api: tweepy.API,\n",
        "                    profile_screen_name: str, \n",
        "                    df_users: pd.DataFrame,\n",
        "                    df_relations: pd.DataFrame):\n",
        "  \"\"\"\n",
        "\n",
        "  :param api: the (initialized) tweepy twitter api istance to use\n",
        "  :param profile_screen_name: the screen name of the user from which I select the 5 randoms\n",
        "  :param df_users:\n",
        "  :param df_relations:\n",
        "  \n",
        "  return: df_random_followings, df_random_followings_relations\n",
        "  \"\"\"\n",
        "\n",
        "  # user_id = df_users.loc[0,\"id\"]\n",
        "  user_id = df_users.loc[df_users[\"screen_name\"] == profile_screen_name, \"id\"].to_numpy()[0]\n",
        "\n",
        "  #code to get 5 random followings\n",
        "\n",
        "  # get ids of all followings of user_id\n",
        "  df_followings = df_relations.loc[df_relations[\"Follower\"] == user_id]\n",
        "\n",
        "  # get their data\n",
        "  df_followings = df_followings.join(df_users, lsuffix='Following', rsuffix='id')\n",
        "\n",
        "  # select only the onces that follows at least 10 people\n",
        "  df_followings = df_followings.loc[df_followings[\"friends_count\"] >= 10]\n",
        "\n",
        "  # get 5 followings random\n",
        "  df_sample = df_followings.sample(n=5)\n",
        "  random_id = df_sample['id'].to_numpy()\n",
        "\n",
        "  # print(random_id)\n",
        "\n",
        "  print(\"Scelti i 5 casuali diretti\")\n",
        "  \n",
        "  #code to get 10 followings for each random following\n",
        "  list_of_ten = [] #the list will have 50 id\n",
        "\n",
        "  # prepare the dataframe where i say who follows who\n",
        "  df_random_followings_relations = pd.DataFrame()\n",
        "\n",
        "  for user in random_id:\n",
        "\n",
        "    # followings = api.friends_ids(user)\n",
        "\n",
        "    followings = list()\n",
        "    \n",
        "    for flw in tweepy.Cursor(api.friends_ids, id=user).pages():\n",
        "      followings.extend(flw)\n",
        "\n",
        "    # print(followings)\n",
        "    \n",
        "    ten = rn.sample(followings, 10)\n",
        "\n",
        "    # print(ten)\n",
        "    for random_follow in ten:\n",
        "      list_of_ten.append(random_follow)\n",
        "      # user -> random_follow\n",
        "      row_relation = pd.DataFrame([[user, random_follow]], columns=[\"Follower\", \"Following\"])\n",
        "      df_random_followings_relations = pd.concat([df_random_followings_relations, row_relation])\n",
        "  \n",
        "  # print(list_of_ten)\n",
        "\n",
        "  print(\"Scelti i profili da scaricare\")\n",
        "  i=0\n",
        "\n",
        "  #code to create a dataframe with the users's info\n",
        "  df_random_followings = pd.DataFrame()\n",
        "\n",
        "  for item in list_of_ten:\n",
        "\n",
        "    user_info = api.get_user(id=item)._json\n",
        "    \n",
        "    # for user in tweepy.Cursor(api.get_user, id=item).items(1):\n",
        "    #  user_info = user._json\n",
        "    \n",
        "    row = pd.json_normalize(user_info)\n",
        "    df_random_followings = pd.concat([df_random_followings, row])\n",
        "\n",
        "    print(i, \"% dei profili scaricati\")\n",
        "    i = i+2\n",
        "\n",
        "  \n",
        "  print(\"Operazione Completata!\")\n",
        "  # I return the new users data + the relations df that says who follows who\n",
        "  return df_random_followings, df_random_followings_relations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOvVxQRjiWhu"
      },
      "source": [
        "Reperimento dati (per uno dei 5 profili):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0NJEMnmiV8N"
      },
      "source": [
        "# Random's Followers and Random's Followings\n",
        "random_users_followers, random_rel_followers = random_followers(api, \"damiano10\", df_users, df_relations)\n",
        "random_users_following, random_rel_following = random_followings(api, \"damiano10\", df_users, df_relations)\n",
        "\n",
        "# concat random users datasets\n",
        "random_users = pd.concat([random_users_followers, random_users_following])\n",
        "\n",
        "# concat random relations datasets\n",
        "random_rel = pd.concat([random_rel_followers, random_rel_following])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6z9cpn7ESz8"
      },
      "source": [
        "# save the datasets in csv\n",
        "random_users.to_csv(data_folder + \"raw_data/df_users_damiano10_5random.csv\", index=False)\n",
        "random_rel.to_csv(data_folder + \"raw_data/df_relations_damiano10_5random.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Lxo4XPiigiV"
      },
      "source": [
        "### Unione dei datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qadw_BxPiuXk"
      },
      "source": [
        "A questo punto, sono stati scaricati, per ognuno dei 5 profili principali:\n",
        "\n",
        "* un dataset `df_user_nome_profilo.csv` con i dati del profilo e dei followers e followed diretti\n",
        "* un dataset `df_relations_nome_profilo.csv` con le indicazioni su chi sono i followers e chi sono i followed\n",
        "* un dataset `df_user_nome_profilo_5random.csv` con i dati dei 5\\*10\\*2 profili random scoperti tramite la ricerca dei 10 dei 5 follower/following random\n",
        "* un dataset `df_relations_profilo_5random.csv` con le indicazioni su di chi sono followers/following questi profili casuali\n",
        "\n",
        "Tutti questi datasets, vengono uniti in 2 unici datasets `df_users.csv` e `df_relations.csv` tramite la funzione `merge_relations_datasets`, che riassume tutti i datasets di una cartella in 2 unici dataframes pandas (poi salvati in csv)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2avvNIDimCE"
      },
      "source": [
        "def merge_users_datasets(data_folder: str):\n",
        "  \"\"\"\n",
        "  Merge all the \"df_users*.csv\" from a certain data folder\n",
        "\n",
        "  :param data_folder: the path to search the datasets\n",
        "  \n",
        "  return: df_users\n",
        "  \"\"\"\n",
        "\n",
        "  df_users = pd.DataFrame()\n",
        "\n",
        "  # Searching alle the file starting with \"df_users_\"\n",
        "  files = os.listdir(data_folder)\n",
        "\n",
        "  for entry in files:\n",
        "    if (\"df_users_\" in os.path.basename(entry)):\n",
        "      df_users = df_users.append(pd.read_csv(data_folder + entry))\n",
        "  \n",
        "  df_users = df_users.drop_duplicates([\"id\"])\n",
        "  df_users = df_users.drop([\"Unnamed: 0\"], axis=1)\n",
        "\n",
        "  print(\"Fine merge datasets df_users!\")\n",
        "  return df_users"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hiwuXrSiyal"
      },
      "source": [
        "def merge_relations_datasets(data_folder: str):\n",
        "  \"\"\"\n",
        "  Merge all the \"df_relations*.csv\" from a certain data folder\n",
        "\n",
        "  :param data_folder: the path to search the datasets\n",
        "  \n",
        "  return: df_relations\n",
        "  \"\"\"\n",
        "\n",
        "  df_relations = pd.DataFrame()\n",
        "\n",
        "  # Searching alle the file starting with \"df_relations_\"\n",
        "  files = os.listdir(data_folder)\n",
        "\n",
        "  for entry in files:\n",
        "    if (\"df_relations_\" in os.path.basename(entry)):\n",
        "      df_relations = df_relations.append(pd.read_csv(data_folder + entry))\n",
        "  \n",
        "  df_relations = df_relations.drop_duplicates([\"Follower\",\"Following\"])\n",
        "\n",
        "  df_relations = df_relations.drop([\"Unnamed: 0\"], axis=1)\n",
        "  print(\"Fine merge datasets df_relations!\")\n",
        "  return df_relations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_avN5107i7mS"
      },
      "source": [
        "# Merge datasets\n",
        "df_users = merge_users_datasets(data_folder + \"raw_data/\")\n",
        "df_relations = merge_relations_datasets(data_folder + \"raw_data/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YvNZTt6niLL"
      },
      "source": [
        "Visualizzazzione datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6xoJCSEnhdA"
      },
      "source": [
        "display(df_users.head())\n",
        "display(df_relations.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YxcYilhm4ie"
      },
      "source": [
        "Salvataggio datasets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "col_k99im4HI"
      },
      "source": [
        "df_users.to_csv(data_folder + \"data/df_users.csv\", index=False)\n",
        "df_relations.to_csv(data_folder + \"data/df_relations.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPzTE3KHfbAb"
      },
      "source": [
        "### Verifica della relazione tra i profili"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2zPdEUrijH6"
      },
      "source": [
        "def check_relations(api: tweepy.API, df_users_source: pd.DataFrame, df_users_target: pd.DataFrame):\n",
        "  \"\"\"\n",
        "  Check the relations between users in df_users_source and users in df_users_target\n",
        "\n",
        "  :param api: the (initialized) tweepy twitter api istance to use\n",
        "  :param df_users_source: \n",
        "  :param df_users_target: \n",
        "\n",
        "  return: a dataframe that explain the relations between all combination of users in \n",
        "          df_users_source and df_users_target\n",
        "  \"\"\"\n",
        "  \n",
        "  # get ids of profiles\n",
        "\n",
        "\n",
        "  # (source, target, following, followed_by)\n",
        "  df_accurate_relations = pd.DataFrame(columns=[\"source\", \"target\", \n",
        "                                                \"following\", \"followed_by\"])\n",
        "  \n",
        "  n_users = df_users_source.shape[0] \n",
        "  i = 0\n",
        "  n_errors = 0\n",
        "\n",
        "  for index,user in df_users_source.iterrows():\n",
        "\n",
        "    id = user[\"id\"]\n",
        "\n",
        "    for index2,compare_w_user in df_users_target.iterrows():\n",
        "      \n",
        "      id_cwu = compare_w_user[\"id\"]\n",
        "\n",
        "      check = df_accurate_relations.loc[\n",
        "          ((df_accurate_relations[\"source\"]==id) & (df_accurate_relations[\"target\"]==id_cwu)) |\n",
        "          ((df_accurate_relations[\"source\"]==id_cwu) & (df_accurate_relations[\"target\"]==id))  \n",
        "      ]\n",
        "\n",
        "      # (Check that the 2 profiles are different and \n",
        "      # that id and id_cwu is not yet compared)\n",
        "      if id != id_cwu and check.shape[0] == 0: \n",
        "        \n",
        "        try:\n",
        "\n",
        "          # check friendship\n",
        "          friendship = api.show_friendship(source_id=id, target_id=id_cwu)[0]\n",
        "  \n",
        "          row = pd.DataFrame([[id, id_cwu, friendship.following, friendship.followed_by]], \n",
        "                              columns=[\"source\", \"target\", \"following\", \"followed_by\"])\n",
        "          df_accurate_relations = pd.concat([df_accurate_relations, row])\n",
        "\n",
        "        except tweepy.error.TweepError:\n",
        "          print(\"Pfiu... salvato in corner...\")\n",
        "          n_errors = n_errors+1\n",
        "        \n",
        "        #(end if)\n",
        "\n",
        "    # (end internal for)\n",
        "\n",
        "    # % complete debug print\n",
        "    i = i+1\n",
        "    print(math.floor(i/n_users*100), \"%\")\n",
        "\n",
        "  # (end external for)\n",
        "\n",
        "  print(\"numero errori: \", n_errors)\n",
        "\n",
        "  return df_accurate_relations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDmjevdrjVce"
      },
      "source": [
        "# get users\n",
        "df_users = pd.read_csv(data_folder + \"data/df_users.csv\")\n",
        "\n",
        "# main 5 users\n",
        "compare_w_profiles_screen_names = [\"mizzaro\", \"damiano10\", \"Miccighel_\", \"eglu81\", \"KevinRoitero\"]\n",
        "\n",
        "# main 5 users details\n",
        "df_compare_w_users = pd.DataFrame(\n",
        "      compare_w_profiles_screen_names, columns=[\"screen_name\"]).merge(\n",
        "          df_users, on=\"screen_name\")\n",
        "      \n",
        "  \n",
        "df_accurate_relations_sample = check_relations(api, df_users.sample(100), df_compare_w_users)\n",
        "\n",
        "df_accurate_relations_sample.to_csv(data_folder + \"data/df_accurate_relations_sample_100.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NdDBHLhjkEq"
      },
      "source": [
        "# check that all relations in df_accurate_relations_sample are yet catched  in df_relations\n",
        "df_accurate_relations_sample = pd.read_csv(data_folder + \"data/df_accurate_relations_sample_100.csv\")\n",
        "\n",
        "# get the relations\n",
        "df_relations = pd.read_csv(data_folder + \"data/df_relations.csv\")\n",
        "\n",
        "n_errors = 0\n",
        "\n",
        "for index, row in df_accurate_relations_sample.iterrows():\n",
        "\n",
        "  source=row[\"source\"]\n",
        "  target = row[\"target\"]\n",
        "  source_follows_target = row[\"following\"]\n",
        "  source_followed_by_target = row[\"followed_by\"]\n",
        "\n",
        "  # search in df_relations ... \n",
        "  source_follows_target_search = df_relations.loc[((df_relations[\"Follower\"]==source) & (df_relations[\"Following\"]==target))].shape[0]>0\n",
        "  source_followed_by_target_search = df_relations.loc[((df_relations[\"Follower\"]==target) & (df_relations[\"Following\"]==source))].shape[0]>0\n",
        "\n",
        "  if not source_follows_target == source_follows_target_search:\n",
        "    print(\"problem: \", source, \" -> \", target, \" = \", source_follows_target)\n",
        "    n_errors = n_errors+1\n",
        "\n",
        "  if not source_followed_by_target == source_followed_by_target_search:\n",
        "    print(\"problem: \", target, \" -> \", source, \" = \", source_followed_by_target)\n",
        "    n_errors = n_errors+1\n",
        "\n",
        "print(n_errors, \"wrong or undetected relations in 100x5 total\", sep=\" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLtwvrMAfod6"
      },
      "source": [
        "## Analisi della rete sociale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvim6fQjfrj1"
      },
      "source": [
        "### Costruzione del grafo della rete sociale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAtFGkl3o7Gp"
      },
      "source": [
        "# read the files\n",
        "df_users = pd.read_csv(data_folder + \"data/df_users.csv\")\n",
        "df_relations = pd.read_csv(data_folder + \"data/df_relations.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r1oQIA9kyDI"
      },
      "source": [
        "def graph_from_df_users_relations(df_users: pd.DataFrame(), \n",
        "                                  df_relations: pd.DataFrame(), \n",
        "                                  directed: bool=True, \n",
        "                                  include_user_data: bool=True, \n",
        "                                  use_screen_name: bool=False):\n",
        "  \"\"\"\n",
        "  Build the graph of twitter users follow-friend relations\n",
        "\n",
        "  :param df_users: the data frame with all the users data\n",
        "  :param df_relations: the data frame with the users relations \n",
        "  :param directed: set False to create an undirected (default <- Directed)\n",
        "  :param include_user_data: set False to non-include user data here\n",
        "  :param use_screen_name: set True to use screen name as node names\n",
        "\n",
        "  return: a networkx DiGraph with all the users as nodes and follow-friend \n",
        "  relations as edges \n",
        "  \"\"\"\n",
        "\n",
        "  \n",
        "\n",
        "  if directed:\n",
        "    twitter_graph = nx.DiGraph()\n",
        "  else:\n",
        "    twitter_graph = nx.Graph()\n",
        "\n",
        "  # add users as nodes\n",
        "  for index, row in df_users.iterrows():\n",
        "\n",
        "    id = row[\"id\"]\n",
        "    screem_name = row[\"screen_name\"]\n",
        "    \n",
        "    # create the node\n",
        "    if use_screen_name:\n",
        "      twitter_graph.add_node(screem_name, label=screem_name) \n",
        "    else:\n",
        "      twitter_graph.add_node(id, label=screem_name) \n",
        "\n",
        "\n",
        "    if include_user_data: \n",
        "      nx.set_node_attributes(twitter_graph, \n",
        "                             {id: {\n",
        "                                 \"details\": row.to_dict(), \n",
        "                                 \"followers_count\": row[\"followers_count\"]\n",
        "                                 }})\n",
        "\n",
        "    # add also users data\n",
        "    # twitter_graph.nodes[id][\"details\"] = \n",
        "\n",
        "    # add the followers numbers\n",
        "    # twitter_graph.nodes[id][\"followers_count\"] = \n",
        "      \n",
        "\n",
        "  # add follow-friend relations as edges\n",
        "  for index, row in df_relations.iterrows():\n",
        "\n",
        "    # x follows y\n",
        "    if use_screen_name:\n",
        "      # get screen names of profiles\n",
        "      x = df_users.loc[df_users[\"id\"] == row[\"Follower\"], \"screen_name\"].to_numpy()[0]\n",
        "      y = df_users.loc[df_users[\"id\"] == row[\"Following\"], \"screen_name\"].to_numpy()[0]\n",
        "    else: \n",
        "      x, y = row[\"Follower\"], row[\"Following\"]\n",
        "    \n",
        "    twitter_graph.add_edge(x, y, weight=1) \n",
        "\n",
        "  # add code authors to the graph\n",
        "  twitter_graph.authors = [\"Emanuele Lena - 142411\", \"Ilaria Fenos - 142494\", \n",
        "                           \"Massimiliano Baldo - 142296\", \"Simone della Pietà - 141995\"]\n",
        "\n",
        "  # todo: fare bene tutto pt. 5 (anche nostri nomi come attributi \n",
        "  # + n_followers per ogni nodo)\n",
        "\n",
        "  return twitter_graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0ObcHvVk2uG"
      },
      "source": [
        "\n",
        "\n",
        "# build the graph\n",
        "twitter_graph = graph_from_df_users_relations(df_users, df_relations)\n",
        "\n",
        "# build also a undirected version (necessary for some analysis)\n",
        "twitter_graph_undirected = graph_from_df_users_relations(df_users, df_relations, directed=False)\n",
        "\n",
        "# build also a version that uses screen names as node names \n",
        "# (for conversion in pyvis)\n",
        "twitter_graph_screen_names = graph_from_df_users_relations(df_users, df_relations, \n",
        "                                                     include_user_data=False, \n",
        "                                                     use_screen_name=True)\n",
        "\n",
        "# (save the graphs in pickle format)\n",
        "# Save in pickle the graph data\n",
        "with open(data_folder + \"networkx_graphs/twitter_graph.pickle\", 'wb') as handle:\n",
        "    pickle.dump(twitter_graph, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open(data_folder + \"networkx_graphs/twitter_graph_undirected.pickle\", 'wb') as handle:\n",
        "    pickle.dump(twitter_graph_undirected, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open(data_folder + \"networkx_graphs/twitter_graph_screen_names.pickle\", 'wb') as handle:\n",
        "    pickle.dump(twitter_graph_screen_names, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "print(\"Grafo generato!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp8h-H00qWRI"
      },
      "source": [
        "### Variabile dove si salva i risultati dell'analisi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUuZMge2lpM6"
      },
      "source": [
        "# where I save the data\n",
        "graph_data = {}\n",
        "try:\n",
        "  with (open(data_folder + \"analysis_data/graph_data.pickle\", \"rb\")) as openfile:\n",
        "      while True:\n",
        "          try:\n",
        "              graph_data.update(pickle.load(openfile))\n",
        "          except:\n",
        "              break\n",
        "  print(\"Found this data=\", graph_data)\n",
        "except:\n",
        "  print(\"No saved data found, create new\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQvPG6SLf0DN"
      },
      "source": [
        "### Caratteristiche generali"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiR6Pz4Xk7sU"
      },
      "source": [
        "print(\"Nnodi: \", twitter_graph.number_of_nodes())\n",
        "print(\"Narchi: \", twitter_graph.number_of_edges())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUiiKz8ll4Z7"
      },
      "source": [
        "Verifica grafo connesso e bipartito"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOs2AFf5llZu"
      },
      "source": [
        "# Verify if graph is connected\n",
        "is_connected = nx.is_connected(twitter_graph_undirected)\n",
        "\n",
        "# Verify if graph is bipartite\n",
        "is_bipartite = nx.is_bipartite(twitter_graph)\n",
        "\n",
        "graph_data[\"is_connected\"] = is_connected\n",
        "graph_data[\"is_bipartite\"] = is_bipartite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cfti0rdolusi"
      },
      "source": [
        "print(\"Connesso?\", is_connected)\n",
        "print(\"Bipartito?\", is_bipartite)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rn15aHhJlzjm"
      },
      "source": [
        "Centro, diametro e raggio:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyqPf39KlwiJ"
      },
      "source": [
        "# (difficoult to calculate)\n",
        "\n",
        "# center = nx.center(twitter_graph)\n",
        "center = nx.center(twitter_graph_undirected)\n",
        "print(\"centro: \", center)\n",
        "\n",
        "graph_data[\"center\"] = center"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7-hHYWOlxvT"
      },
      "source": [
        "# (semi-difficoult to calculate)\n",
        "\n",
        "# diameter = nx.diameter(twitter_graph)\n",
        "diameter = nx.diameter(twitter_graph_undirected)\n",
        "print(\"diametro: \", diameter)\n",
        "\n",
        "graph_data[\"diameter\"] = diameter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lfd6WjA6ly3V"
      },
      "source": [
        "# (semi-difficoult to calculate)\n",
        "\n",
        "# radius = nx.radius(twitter_graph)\n",
        "radius = nx.radius(twitter_graph_undirected)\n",
        "print(\"raggio: \", radius)\n",
        "\n",
        "graph_data[\"radius\"] = radius"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9f2-baKmGIq"
      },
      "source": [
        "Nodi del centro:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohQd0lR3mFv2"
      },
      "source": [
        "for node_id in graph_data[\"center\"]:\n",
        "  name = twitter_graph.nodes[node_id][\"label\"]\n",
        "  print(node_id, name, sep=\"\\t\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cs30DcOf3z8"
      },
      "source": [
        "### Visualizzazione della rete"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIVafde7lYNk"
      },
      "source": [
        "Visualizzazione con draw_network (pessimi risultati)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7nMsoKElQtY"
      },
      "source": [
        "rcParams['figure.figsize'] = 50,50\n",
        "nx.draw_networkx(\n",
        "    twitter_graph,\n",
        "    pos = nx.spring_layout(twitter_graph),\n",
        "    node_color=\"#A0CBE2\",\n",
        "    width=2,\n",
        "    edge_cmap = plt.cm.Blues,\n",
        "    with_labels = True\n",
        ")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr9W8rrPlaLR"
      },
      "source": [
        "Visualizzazione con pyvis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqY0uKPjk9-m"
      },
      "source": [
        "!pip install pyvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djr6B8RWk_oD"
      },
      "source": [
        "from pyvis.network import Network\n",
        "\n",
        "nt = Network(\n",
        "    height=\"100%\", width=\"100%\", bgcolor=\"#222222\", \n",
        "    font_color=\"white\", heading=\"Twitter Graph Social Computing\"\n",
        ")\n",
        "\n",
        "nt.barnes_hut()\n",
        "nt.from_nx(twitter_graph_screen_names)\n",
        "neighbor_map = nt.get_adj_list()\n",
        "for node in nt.nodes:\n",
        "  node[\"value\"] = len(neighbor_map[node[\"id\"]])\n",
        "\n",
        "# vediamo...\n",
        "nt.show(data_folder + \"pyvise_graphs/twitter_graph.html\")\n",
        "# from google.colab import files\n",
        "# files.download(data_folder + \"pyvise_graphs/twitter_graph.html\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6UBLfgqgCZX"
      },
      "source": [
        "### Misure della centralità"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3blJ_XrcmHA2"
      },
      "source": [
        "* Betweenness centrality (betweenness_centrality)\n",
        "* Closeness centrality (closeness_centrality)\n",
        "* Degree centrality (degree_centrality)\n",
        "* In-degree centrality (in_degree_centrality)\n",
        "* Out-degree centrality (out_degree_centrality)\n",
        "* Page Rank (pagerank)\n",
        "* HITS (hits)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv5x7uoAmGxY"
      },
      "source": [
        "# 9. Calcolate le seguenti misure di centralità sul grafo\n",
        "# Più casini perchè c'è il discorso bipartite\n",
        "# https://networkx.org/documentation/stable/reference/algorithms/bipartite.html#module-networkx.algorithms.bipartite\n",
        "\n",
        "# betweenness_centrality = nx.betweenness_centrality(twitter_graph, list_node_bipartite)\n",
        "# closeness_centrality = nx.closeness_centrality(twitter_graph, list_node_bipartite)\n",
        "# degree_centrality = nx.degree_centrality(twitter_graph, list_node_bipartite)\n",
        "\n",
        "# todo: sistemare\n",
        "\n",
        "# (semi-difficoult to calculate)\n",
        "\n",
        "betweenness_centrality = nx.betweenness_centrality(twitter_graph)\n",
        "\n",
        "print(\"Betweenness calcolata\")\n",
        "graph_data[\"betweenness_centrality\"] = betweenness_centrality\n",
        "\n",
        "closeness_centrality = nx.closeness_centrality(twitter_graph)\n",
        "\n",
        "print(\"Closeness calcolata\")\n",
        "graph_data[\"closeness_centrality\"] = closeness_centrality\n",
        "\n",
        "degree_centrality = nx.degree_centrality(twitter_graph_undirected)\n",
        "\n",
        "print(\"Degree Centrality calcolata\")\n",
        "graph_data[\"degree_centrality\"] = degree_centrality\n",
        "\n",
        "in_degree_centrality = nx.in_degree_centrality(twitter_graph)\n",
        "out_degree_centrality = nx.out_degree_centrality(twitter_graph)\n",
        "\n",
        "print(\"In-Degree e Out-Degree calcolati\")\n",
        "graph_data[\"in_degree_centrality\"] = in_degree_centrality\n",
        "graph_data[\"out_degree_centrality\"] = out_degree_centrality\n",
        "\n",
        "pagerank = nx.pagerank(twitter_graph)\n",
        "\n",
        "print(\"Pagerank calcolato\")\n",
        "graph_data[\"pagerank\"] = pagerank"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ47DYYomPIS"
      },
      "source": [
        "start_time = datetime.now()\n",
        "print('Started at: {}'.format(start_time))\n",
        "\n",
        "hits = nx.hits(twitter_graph, max_iter=500) # todo: sistema\n",
        "\n",
        "end_time = datetime.now()\n",
        "print('Ended at: {}'.format(end_time))\n",
        "\n",
        "graph_data[\"hits\"] = hits\n",
        "\n",
        "print('Duration: {}'.format(end_time - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP8vFEcSmZge"
      },
      "source": [
        "print(\"Betweenness centrality: \", betweenness_centrality)\n",
        "print(\"Closeness centrality \", closeness_centrality)\n",
        "print(\"Degree centrality \", degree_centrality)\n",
        "print(\"In-degree centrality \", in_degree_centrality)\n",
        "print(\"Out-degree centrality \", out_degree_centrality)\n",
        "print(\"Page Rank \", pagerank)\n",
        "print(\"HITS \", hits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQLmuDi1Jsol"
      },
      "source": [
        "### Visualzzione del miglior per ogni misura di centralità"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIyKGz4exiob"
      },
      "source": [
        "# Create the dataframe for every measure of centrality, which are stored in the graph_data dictionary\n",
        "df_betweenness = pd.DataFrame(graph_data[\"betweenness_centrality\"].items(), columns=['id', 'betweenness_centrality'])\n",
        "df_degree = pd.DataFrame(graph_data[\"degree_centrality\"].items(), columns=['id', 'degree_centrality'])\n",
        "df_in_degree = pd.DataFrame(graph_data[\"in_degree_centrality\"].items(), columns=['id', 'in_degree_centrality'])\n",
        "df_out_degree = pd.DataFrame(graph_data[\"out_degree_centrality\"].items(), columns=['id', 'out_degree_centrality'])\n",
        "df_pagerank = pd.DataFrame(graph_data[\"pagerank\"].items(), columns=['id', 'pagerank'])\n",
        "df_hits_hubness = pd.DataFrame(graph_data[\"hits\"][0].items(), columns=['id', 'hits_hubness'])\n",
        "df_hits_autority = pd.DataFrame(graph_data[\"hits\"][1].items(), columns=['id', 'hits_authority'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmUG6E-uxjTI"
      },
      "source": [
        "# For all the measure, sort the dataframe and take the first element\n",
        "df_betweenness = df_betweenness.sort_values(by='betweenness_centrality', ascending=False)\n",
        "df_betweenness = df_betweenness.head(1)\n",
        "# Now for find the name of that id, use the join function with the df_user.\n",
        "# Using the dataframes in this case is more cheap, becuase all the label you need \n",
        "# are stored in column of the dataframe. Using the graph, you need to search all this stuff.\n",
        "df_betweenness = df_betweenness.join(df_users.set_index(\"id\"), on='id')\n",
        "\n",
        "display(df_betweenness[[\"id\", \"name\", \"betweenness_centrality\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU_xil1Lxn_3"
      },
      "source": [
        "df_degree = df_degree.sort_values(by='degree_centrality', ascending=False)\n",
        "df_degree = df_degree.head(1)\n",
        "df_degree = df_degree.join(df_users.set_index(\"id\"), on='id')\n",
        "\n",
        "display(df_degree[[\"id\", \"name\", \"degree_centrality\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7_EFvPYnVGp"
      },
      "source": [
        "df_in_degree = df_in_degree.sort_values(by='in_degree_centrality', ascending=False)\n",
        "df_in_degree = df_in_degree.head(1)\n",
        "df_in_degree = df_in_degree.join(df_users.set_index(\"id\"), on='id')\n",
        "\n",
        "display(df_in_degree[[\"id\", \"name\", \"in_degree_centrality\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWTWQsW6nXEW"
      },
      "source": [
        "df_out_degree = df_out_degree.sort_values(by='out_degree_centrality', ascending=False)\n",
        "df_out_degree = df_out_degree.head(1)\n",
        "df_out_degree = df_out_degree.join(df_users.set_index(\"id\"), on='id')\n",
        "\n",
        "display(df_out_degree[[\"id\", \"name\", \"out_degree_centrality\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIiJcrDwnZ_j"
      },
      "source": [
        "df_pagerank = df_pagerank.sort_values(by='pagerank', ascending=False)\n",
        "df_pagerank = df_pagerank.head(1)\n",
        "df_pagerank = df_pagerank.join(df_users.set_index(\"id\"), on='id')\n",
        "\n",
        "display(df_pagerank[[\"id\", \"name\", \"pagerank\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f28J2d4nbt3"
      },
      "source": [
        "df_hits_hubness = df_hits_hubness.sort_values(by='hits_hubness', ascending=False)\n",
        "df_hits_hubness = df_hits_hubness.head(1)\n",
        "df_hits_hubness = df_hits_hubness.join(df_users.set_index(\"id\"), on='id')\n",
        "\n",
        "display(df_hits_hubness[[\"id\", \"name\", \"hits_hubness\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEB-lrnknfRf"
      },
      "source": [
        "df_hits_autority = df_hits_autority.sort_values(by='hits_authority', ascending=False)\n",
        "df_hits_autority = df_hits_autority.head(1)\n",
        "df_hits_autority = df_hits_autority.join(df_users.set_index(\"id\"), on='id')\n",
        "\n",
        "display(df_hits_autority[[\"id\", \"name\", \"hits_authority\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sTQHGpagQgv"
      },
      "source": [
        "### Calcolo del sotto-grafo ridotto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBVbgR8ZpN-3"
      },
      "source": [
        "# get KevinRoitero's id\n",
        "id_sample_profile = df_users.loc[df_users[\"screen_name\"] == \"KevinRoitero\", \"id\"].to_numpy()[0]\n",
        "\n",
        "# calculate subgraph\n",
        "# reduce_graph = nx.ego_graph(twitter_graph, id_sample_profile)\n",
        "reduce_graph = nx.ego_graph(twitter_graph_undirected, id_sample_profile)\n",
        "\n",
        "print(\"Calcolato sottografo indotto!\")\n",
        "print(\"Nnodes: \", reduce_graph.number_of_nodes())\n",
        "print(\"Nedges: \", reduce_graph.number_of_edges())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-y0tU2fgVz9"
      },
      "source": [
        "### Calcolo della cricca massima"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrRLA3tUpP0T"
      },
      "source": [
        "# calculate max clique \n",
        "from networkx.algorithms.approximation import clique\n",
        "\n",
        "large_clique_size = clique.large_clique_size(reduce_graph)\n",
        "print(\"Dimensione: \", large_clique_size)\n",
        "graph_data[\"large_clique_size\"] = large_clique_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcxx0i6IpQ70"
      },
      "source": [
        "# (difficoult to compute)\n",
        "max_clique = clique.max_clique(reduce_graph)\n",
        "graph_data[\"max_clique\"] = max_clique"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yApD9L3pSCu"
      },
      "source": [
        "print(\"Cricca massima: \", graph_data[\"max_clique\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6homFPripWll"
      },
      "source": [
        "ricavo i nomi dei componenti della cricca: \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjalF1etpTLF"
      },
      "source": [
        "clique_ids = np.fromiter(graph_data[\"max_clique\"], int, len(graph_data[\"max_clique\"]))\n",
        "clique_names = pd.DataFrame(clique_ids, columns=[\"id\"])\n",
        "clique_names = clique_names.merge(df_users, on=\"id\")\n",
        "clique_names[[\"id\", \"screen_name\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cJEH4PFlxSq"
      },
      "source": [
        "(Versione alternativa per recuperare i nomi dove non si usano i dataset)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3cUw6c1lX18"
      },
      "source": [
        "for node_id in graph_data[\"max_clique\"]:\n",
        "  name = twitter_graph.nodes[node_id][\"label\"]\n",
        "  print(node_id, name, sep=\"\\t\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfmg6ikspZTX"
      },
      "source": [
        "ricavo il sottografo con la cricca: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqS0bMqYpbP8"
      },
      "source": [
        "clique_graph = twitter_graph.subgraph(clique_ids)\n",
        "clique_graph\n",
        "\n",
        "rcParams['figure.figsize'] = 15,5\n",
        "nx.draw_networkx(\n",
        "    clique_graph,\n",
        "    pos = nx.spring_layout(clique_graph),\n",
        "    node_color=\"#A0CBE2\",\n",
        "    width=2,\n",
        "    edge_cmap = plt.cm.Blues,\n",
        "    with_labels = True\n",
        ")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoipVUt3gYZ9"
      },
      "source": [
        "### Calcolo della copertura minima"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9mPe718ph-y"
      },
      "source": [
        "# min_edge_cover = nx.min_edge_cover(twitter_graph)\n",
        "min_edge_cover = nx.min_edge_cover(twitter_graph_undirected)\n",
        "print(\"Copertura minima degli archi: \", min_edge_cover)\n",
        "graph_data[\"min_edge_cover\"] = min_edge_cover"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ2bMQ6Eghnq"
      },
      "source": [
        "### Stima della “small-world-ness” del grafo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3XqSKcIp5We"
      },
      "source": [
        "Definizioni da documentazione ufficiale:\n",
        "\n",
        "*   Omega\n",
        "\n",
        "> \"The small-world coefficient of a graph G is:\n",
        "\n",
        "> omega = Lr/L - C/Cl\n",
        "\n",
        "> where C and L are respectively the average clustering coefficient and average shortest path length of G. Lr is the average shortest path length of an equivalent random graph and Cl is the average clustering coefficient of an equivalent lattice graph.\n",
        "\n",
        "> The small-world coefficient (omega) ranges between -1 and 1. Values close to 0 means the G features small-world characteristics. Values close to -1 means G has a lattice shape whereas values close to 1 means G is a random graph.\"\n",
        "\n",
        "*   Sigma\n",
        "\n",
        "> \"The small-world coefficient is defined as: sigma = C/Cr / L/Lr where C and L are respectively the average clustering coefficient and average shortest path length of G. Cr and Lr are respectively the average clustering coefficient and average shortest path length of an equivalent random graph. A graph is commonly classified as small-world if sigma>1.\"\n",
        "\n",
        "Quindi secondo l'Omega ricavato, la rete è piccolo mondo. Secondo Sigma invece no (ma ci siamo vicini)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPwqCnneplaX"
      },
      "source": [
        "# (very difficoult to compute)\n",
        "\n",
        "start_time = datetime.now()\n",
        "print('Started at: {}'.format(start_time))\n",
        "\n",
        "# omega = nx.omega(twitter_graph)\n",
        "omega = nx.omega(twitter_graph_undirected, niter=1, nrand=1)\n",
        "# omega = nx.omega(reduce_graph)\n",
        "\n",
        "end_time = datetime.now()\n",
        "print('Ended at: {}'.format(end_time))\n",
        "\n",
        "graph_data[\"omega\"] = omega\n",
        "\n",
        "print('Duration: {}'.format(end_time - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7nSW6qzpzFx"
      },
      "source": [
        "# (difficoult to compute)\n",
        "\n",
        "start_time = datetime.now()\n",
        "print('Started at: {}'.format(start_time))\n",
        "\n",
        "# sigma = nx.sigma(twitter_graph)\n",
        "sigma = nx.sigma(twitter_graph_undirected, niter=1, nrand=1)\n",
        "# sigma = nx.sigma(reduce_graph, niter=1)\n",
        "\n",
        "end_time = datetime.now()\n",
        "print('Ended at: {}'.format(end_time))\n",
        "\n",
        "graph_data[\"sigma\"] = sigma\n",
        "\n",
        "print('Duration: {}'.format(end_time - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab_cRfUPp0U0"
      },
      "source": [
        "print(\"omega (grafo ridotto): \", graph_data[\"omega\"] )\n",
        "print(\"sigma (grafo ridotto): \", graph_data[\"sigma\"] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcD27MabgF9-"
      },
      "source": [
        "### Correlazione tra le misure di centralità"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oWlIBW8pCbO"
      },
      "source": [
        "df_measure = pd.DataFrame()\n",
        "\n",
        "# Recreate the dataframe used previously\n",
        "df_betweenness = pd.DataFrame(graph_data[\"betweenness_centrality\"].items(), columns=['id', 'betweenness_centrality'])\n",
        "df_degree = pd.DataFrame(graph_data[\"degree_centrality\"].items(), columns=['id', 'degree_centrality'])\n",
        "df_in_degree = pd.DataFrame(graph_data[\"in_degree_centrality\"].items(), columns=['id', 'in_degree_centrality'])\n",
        "df_out_degree = pd.DataFrame(graph_data[\"out_degree_centrality\"].items(), columns=['id', 'out_degree_centrality'])\n",
        "df_pagerank = pd.DataFrame(graph_data[\"pagerank\"].items(), columns=['id', 'pagerank'])\n",
        "df_hits_hubness = pd.DataFrame(graph_data[\"hits\"][0].items(), columns=['id', 'hits_hubness'])\n",
        "df_hits_autority = pd.DataFrame(graph_data[\"hits\"][1].items(), columns=['id', 'hits_authority'])\n",
        "\n",
        "# To create a dataframe which contains all the measures, you need to concatenate\n",
        "# multiple join operations using the \"id\" for the key of join, and for that reason\n",
        "# the operation must be executed every single step \n",
        "df_measure = df_betweenness.join(df_degree.set_index(\"id\"), on='id')\n",
        "df_measure = df_measure.join(df_in_degree.set_index(\"id\"), on='id')\n",
        "df_measure = df_measure.join(df_out_degree.set_index(\"id\"), on='id')\n",
        "df_measure = df_measure.join(df_pagerank.set_index(\"id\"), on='id')\n",
        "df_measure = df_measure.join(df_hits_hubness.set_index(\"id\"), on='id')\n",
        "df_measure = df_measure.join(df_hits_autority.set_index(\"id\"), on='id')\n",
        "\n",
        "display(df_measure)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4fxxbPZpEfk"
      },
      "source": [
        "# Create the dataframe where it will store the correlation values\n",
        "df_correlation_rho = pd.DataFrame(index = [\"betweenness_centrality\", \"degree_centrality\", \"in_degree_centrality\", \"out_degree_centrality\", \"pagerank\", \"hits_hubness\", \"hits_authority\"],\n",
        "                              columns = [\"betweenness_centrality\", \"degree_centrality\", \"in_degree_centrality\", \"out_degree_centrality\", \"pagerank\", \"hits_hubness\", \"hits_authority\"])\n",
        "\n",
        "# Cycling on columns (exclued the \"id\"), you calculate\n",
        "# the correlation value with the other column and save the file\n",
        "# in a Series, which will be added in the final dataframe\n",
        "for col1 in df_measure.columns:\n",
        "  if (col1 != \"id\"):\n",
        "    new_col = pd.Series(dtype = \"float64\")\n",
        "    for col2 in df_measure.columns:\n",
        "      if (col2 != \"id\"):\n",
        "        r, p = stats.pearsonr(df_measure[col1], df_measure[col2])\n",
        "        new_col = new_col.append(pd.Series(r), ignore_index=True)\n",
        "    df_correlation_rho[col1] = new_col.values\n",
        "\n",
        "# The final result will be a symmetrical dataframe on the main diagonal\n",
        "display(df_correlation_rho)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2j_UxoLpF2l"
      },
      "source": [
        "df_correlation_tau = pd.DataFrame(index = [\"betweenness_centrality\", \"degree_centrality\", \"in_degree_centrality\", \"out_degree_centrality\", \"pagerank\", \"hits_hubness\", \"hits_authority\"],\n",
        "                              columns = [\"betweenness_centrality\", \"degree_centrality\", \"in_degree_centrality\", \"out_degree_centrality\", \"pagerank\", \"hits_hubness\", \"hits_authority\"])\n",
        "\n",
        "for col1 in df_measure.columns:\n",
        "  if (col1 != \"id\"):\n",
        "    new_col = pd.Series(dtype = \"float64\")\n",
        "    for col2 in df_measure.columns:\n",
        "      if (col2 != \"id\"):\n",
        "        tau, p_value = stats.kendalltau(df_measure[col1], df_measure[col2])\n",
        "        new_col = new_col.append(pd.Series(tau), ignore_index=True)\n",
        "    df_correlation_tau[col1] = new_col.values\n",
        "\n",
        "display(df_correlation_tau)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PZTES43pGyT"
      },
      "source": [
        "df_correlation_rho.to_csv(data_folder + \"analysis_data/df_correlation_rho.csv\", index=False)\n",
        "df_correlation_tau.to_csv(data_folder + \"analysis_data/df_correlation_tau.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dkMwyuTp_0Q"
      },
      "source": [
        "### Salvataggio in pickle dei risultati dell'analisi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftfjkPdQp-7f"
      },
      "source": [
        "print(graph_data)\n",
        "\n",
        "# Save in pickle the graph data\n",
        "with open(data_folder + \"analysis_data/graph_data.pickle\", 'wb') as handle:\n",
        "    pickle.dump(graph_data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}